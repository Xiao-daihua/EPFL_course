\subsection{Classical Entropy}
\subsubsection{Single Random Variable Entropy}


\hlr{Shannon Entropy}

首先我们定义经典信息理论之中的熵的概念：
\defi{
  Shannon Entropy

  对于一个随机变量$ X $其可能取值为$ \{x_i\} $，对应的概率分布为$ \{p_i\} $，我们定义这个随机变量的Shannon Entropy为：
  \begin{align}
    H(X) = -\sum_i p_i \log p_i.
  \end{align}
  注意！对于经典信息理论这里的对数我们一般取以2为底！
}


